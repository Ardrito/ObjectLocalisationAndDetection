{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd6a89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardrit/MLX/week_6/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "# === Set up environment ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load CLIP & Tokenizer ===\n",
    "CLIP = transformers.CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "tokenizer = transformers.CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "CLIP.eval()\n",
    "for param in CLIP.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "token_embedding = CLIP.text_model.embeddings.token_embedding.to(device)\n",
    "\n",
    "# === Load token embedding ===\n",
    "token_embedding.weight.requires_grad = False\n",
    "\n",
    "flickr = load_from_disk(\"flickr30k_dataset/\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "# === Decoder Layer & Decoder (Same as in training) ===\n",
    "class decoder_layer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, output_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 2 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.size()\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf')).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        scores = scores + mask\n",
    "        A = torch.softmax(scores, dim=-1)\n",
    "        H = torch.matmul(A, V).transpose(1, 2).reshape(B, T, self.embed_dim)\n",
    "        H = self.out_proj(H)\n",
    "\n",
    "        x = self.norm1(x + H)\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, output_dim, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.patchPRoj = nn.Linear(768, embed_dim)\n",
    "        self.decoder_layers = nn.ModuleList([decoder_layer(embed_dim, num_heads, output_dim) for _ in range(num_layers)])\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.reg_out = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def get_positional_encoding(self, seq_len, dim):\n",
    "        pe = torch.zeros(seq_len, dim)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, patch_embedding, output_embedding):\n",
    "        projected_patch = self.patchPRoj(patch_embedding)\n",
    "        x = torch.cat((projected_patch, output_embedding), dim=1)\n",
    "\n",
    "        seq_len = x.size(1)\n",
    "        pe = self.get_positional_encoding(seq_len, x.size(-1)).to(x.device)\n",
    "        x = self.ln1(x + pe.unsqueeze(0))\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return self.reg_out(x)\n",
    "\n",
    "# === Instantiate model ===\n",
    "model = decoder(embed_dim=512, output_dim=len(tokenizer), num_heads=4, num_layers=2).to(device)\n",
    "\n",
    "# === Load weights ===\n",
    "checkpoint = torch.load(\"checkpoints_4/best_model.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "def get_test_image(idx):\n",
    "    test_image = flickr['test'][idx]['image']\n",
    "    # test_cap = flickr['test'][0]['caption'][0]\n",
    "    plt.imshow(test_image)\n",
    "\n",
    "    # test_cap = tokenizer(test_cap,return_tensors=\"pt\", padding=\"max_length\", truncation=True)['input_ids']\n",
    "\n",
    "    # test_cap = token_embedding(test_cap)\n",
    "\n",
    "    # print (test_cap.shape)\n",
    "\n",
    "\n",
    "    preprocess = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.4815, 0.4578, 0.4082],\n",
    "            std=[0.2686, 0.2613, 0.2758]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "    CLIP.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vision_outputs = CLIP.vision_model(preprocess(test_image).unsqueeze(0).to(device))\n",
    "        patch_embeddings = vision_outputs.last_hidden_state[:, 1:, :]\n",
    "\n",
    "    # print (patch_embeddings.shape)\n",
    "\n",
    "    img = patch_embeddings\n",
    "\n",
    "    return (img)\n",
    "\n",
    "# === Inference Function ===\n",
    "\n",
    "def inference(model, idx, start_token=49406, end_token=49407, max_len=77, device='cuda'):\n",
    "    model.eval()\n",
    "     # Shape: (1, 4, 196)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Start with <start> token\n",
    "        generated = [start_token]\n",
    "        #img = get_test_image(idx)\n",
    "        img = idx\n",
    "        for _ in range(max_len):\n",
    "            y = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(0)  # (1, seq_len)\n",
    "\n",
    "            y_emb = token_embedding(y).to(device)\n",
    "            # print (y_emb.shape)\n",
    "            # print (img.shape)\n",
    "            \n",
    "            logits = model(img, y_emb)# (1, seq_len, vocab_size)\n",
    "            next_token_logits = logits[0, -1]  # (vocab_size,)\n",
    "            next_token = torch.argmax(next_token_logits).item()\n",
    "            #print (next_token)\n",
    "            \n",
    "            generated.append(next_token)\n",
    "            \n",
    "            if next_token == end_token:\n",
    "                break\n",
    "\n",
    "    return generated[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d09203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to capture image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@131.205] global cap_v4l.cpp:913 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@131.205] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "image_path = \"example.jpg\"  # replace with your test image\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Wait for the camera to warm up and read one frame\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if ret:\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(frame_rgb)\n",
    "    plt.imshow(pil_image)\n",
    "\n",
    "\n",
    "    preprocess = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.4815, 0.4578, 0.4082],\n",
    "            std=[0.2686, 0.2613, 0.2758]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "    CLIP.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vision_outputs = CLIP.vision_model(preprocess(pil_image).unsqueeze(0).to(device))\n",
    "        patch_embeddings = vision_outputs.last_hidden_state[:, 1:, :]\n",
    "\n",
    "    # print (patch_embeddings.shape)\n",
    "\n",
    "    img = patch_embeddings\n",
    "    caption = inference(model,img)\n",
    "\n",
    "    print(\"\\nðŸ“· Generated caption:\", tokenizer.decode(caption))\n",
    "\n",
    "else:\n",
    "    print(\"Failed to capture image\")\n",
    "    #time.sleep(2)\n",
    "\n",
    "# Release the camera\n",
    "cap.release()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
